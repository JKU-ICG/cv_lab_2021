{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 364.0{21,22,41-45} Introduction to Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you have learned how to manipulate matrices with numpy and some basic image processing with scikit-image processing package. \n",
    "\n",
    "Today we would look into OpenCV (Open source Computer Vision library), use it for a couple of cool image processing application and than demonstrate how to use it for a  computer vision applications.\n",
    "\n",
    "First we import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to install OpenCV-Python version using \n",
    "\n",
    "> `pip install opencv-python` \n",
    "or \n",
    "> `pip install opencv-contrib-python`\n",
    "\n",
    "Run these commands on your terminal or if you are using Anaconda Navigator – Jupyter Notebook, you can change “pip” with the “conda” command and install the same.\n",
    "\n",
    "A detailed overview for opencv [installation](https://docs.opencv.org/master/d0/d3d/tutorial_general_install.html) is provided. You could also get useful information for [python installation](https://pypi.org/project/opencv-python/) here.\n",
    "\n",
    "It also has extensive [documentation](https://docs.opencv.org/master/) and [tutorials](https://docs.opencv.org/master/d9/df8/tutorial_root.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Open-CV library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exist a nice utility library `imutils` which can help you get OpenCV parameters and function names even from partial strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "imutils.find_function('read')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed description, you could always rely on `help`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(cv2.imread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Domain Image Manipulation\n",
    "\n",
    "As you have already learned in the computer vision lecture, gradient based operators operate on the gradient between neighboring pixels rather than the pixel value itself.\n",
    "\n",
    "The goal of gradient domain image manipulation is to construct a new image which minimizes the divergence of image gradients between the images.\n",
    "\n",
    "This is generally done by solving a Poisson equation [1] \n",
    "$$\\triangledown^2 I = div(G)$$\n",
    " which we are not going into detail but we would use to composite two images\n",
    "\n",
    "[1] Patrick Pérez, Michel Gangnet, and Andrew Blake. Poisson image editing. In ACM Transactions on Graphics (TOG), volume 22, pages 313–318. ACM, 2003."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Compositing\n",
    "\n",
    "Compositing an image is a common task in image editing in which some or all of one image is pasted into the another image.\n",
    "\n",
    "First we load both images, for this you  could use a familiar function  i.e imread that you used in scikit to load the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images\n",
    "src = cv2.cvtColor(cv2.imread(\"aeroplane2.jpg\"),cv2.COLOR_BGR2RGB) ##Read images using imread function and convert color space using cvtColor\n",
    "dst = cv2.cvtColor(cv2.imread(\"sp3.jpg\"),cv2.COLOR_BGR2RGB)\n",
    "resultdst = dst.copy()\n",
    "#display settings\n",
    "margin=10 # pixels\n",
    "spacing =35 # pixels\n",
    "dpi=100. # dots per inch\n",
    "\n",
    "width = (src.shape[1]+dst.shape[1]+2*margin+spacing)/dpi # inches\n",
    "height= (max(src.shape[0],dst.shape[0])+2*margin+spacing)/dpi\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(width,height), dpi=dpi)\n",
    "ax[0].imshow(src)\n",
    "ax[1].imshow(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the images we take help of the matplotlib `PolygonSelector` function to create a mask to separate the object from the image which we want to add to the new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.widgets import PolygonSelector\n",
    "def finished(x):\n",
    "    print(\"Selected points:\", x)\n",
    "    plt.close(fig)\n",
    "selectpolygon = False\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(src)\n",
    "if selectpolygon:\n",
    "    ps = PolygonSelector(ax, onselect=finished) #return points from the polygon selecter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting the vertices of the polygon defining the mask, we now generate a binary mask containing the selected object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectpolygon:\n",
    "    points = np.array(ps.verts).astype(np.int32) #convert polygon point vertices to integer array\n",
    "    mask = np.zeros(src.shape, src.dtype)\n",
    "    cv2.fillPoly(mask, [points], (255, 255, 255)) #fill polygon with white color\n",
    "else :\n",
    "    mask = cv2.imread(\"aeroplane2_mask.jpg\") #if polygon selector is not working for some reason load a predefined mask\n",
    "print(mask.shape)\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we would do a naive image editing based on a mask\n",
    "\n",
    "Note that in OpenCV you could still acces each pixel of the image as image_name.at(row,column).\n",
    "\n",
    "However, when you describe a point in OpenCV you need to describe as Point_name(column, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask = cv2.normalize(mask.astype('float'), None, 0.0, 1.0, norm_type=cv2.NORM_MINMAX) #normalize the mask to be between 1 and 0\n",
    "pos = (350,120) # position on the dst image where the object from source image needs to be edited-- (column,row) --- since input to OpenCV require it in this format\n",
    "halfwidth = (int(src_mask.shape[0]/2),int(src_mask.shape[1]/2)) #calculate mask half width\n",
    "###Remember in OpenCV  src.at(i,j) is using (i,j) as (row,column) but Point(x,y) is using (x,y) as (column,row)\n",
    "resultdst[pos[1]-halfwidth[0]:pos[1]+halfwidth[0],pos[0]-halfwidth[1]:pos[0]+ halfwidth[1],:] = dst[pos[1]- halfwidth[0]:pos[1]+halfwidth[0],pos[0]-halfwidth[1]:pos[0]+halfwidth[1],:] * (1 - src_mask) + src * src_mask\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(resultdst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you could see this is not a very good image editing. \n",
    "\n",
    "You could clearly see the seams that is present where you added two different images.\n",
    "\n",
    "Next we will see how this could be done more efficiently by using gradient image manipulation.\n",
    "\n",
    "OpenCV has a very simple implementation to achieve this.\n",
    "We will use the `seamlessClone` function to add the masked object onto the source image. This function implements poisson image blending presented in [1]. \n",
    "\n",
    "However, this is not a very efficient implementation and I hope that you would try to implement a better one for the Bonus task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone seamlessly\n",
    "output = cv2.seamlessClone(src, dst, mask, pos, cv2.MIXED_CLONE)\n",
    "\n",
    "### Poisson blending -- implement yourself\n",
    "# Save result\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(output)\n",
    "cv2.imwrite(\"opencv_gradientdomain_imagemanipulation.png\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Segmentation\n",
    "\n",
    "Defining a better mask for segmenting an object in an image is a time consuming and tedious process.\n",
    "\n",
    "Next we would see how we can use 'Grabcut', a graph based image segmentation algorithm which would do the heavy lifting  of creating a segmentation mask for us.\n",
    "\n",
    "Here, you just define a rectangle containing all objects to be segmented in the image.\n",
    "\n",
    "For this we would take help of matplotlib `RectanlgeSelector`function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.widgets import RectangleSelector\n",
    "def line_select_callback(eclick, erelease):\n",
    "    'eclick and erelease are the press and release events'\n",
    "    global x1, y1, x2, y2\n",
    "    x1, y1 = eclick.xdata, eclick.ydata\n",
    "    x2, y2 = erelease.xdata, erelease.ydata\n",
    "    print(\"(%3.2f, %3.2f) --> (%3.2f, %3.2f)\" % (x1, y1, x2, y2))\n",
    "    print(\" The button you used were: %s %s\" % (eclick.button, erelease.button))\n",
    "\n",
    "def toggle_selector(event):\n",
    "    print(' Key pressed.')\n",
    "    if event.key == 't':\n",
    "        if toggle_selector.RS.active:\n",
    "            print(' RectangleSelector deactivated.')\n",
    "            toggle_selector.RS.set_active(False)\n",
    "        else:\n",
    "            print(' RectangleSelector activated.')\n",
    "            toggle_selector.RS.set_active(True)\n",
    "# ============================================================================\n",
    "\n",
    "##Select object to be segmented, select a rectangle around the object\n",
    "##everything else in the image becomes background\n",
    "fig, current_ax = plt.subplots()\n",
    "plt.imshow(src)\n",
    "RectangleSelection = False\n",
    "if RectangleSelection:\n",
    "    toggle_selector.RS = RectangleSelector(current_ax, line_select_callback,\n",
    "                                        drawtype='box', useblit=True,\n",
    "                                        button=[1, 3],  # don't use middle button\n",
    "                                        minspanx=5, minspany=5,\n",
    "                                        spancoords='pixels',\n",
    "                                        interactive=True)\n",
    "    plt.connect('key_press_event', toggle_selector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm does an initial labelling based on the provided mask. It labels the foreground and background pixels.\n",
    "\n",
    "Now a Gaussian Mixture Model(GMM) is used to model the foreground and background.\n",
    "\n",
    "GMM learns and create new pixel distribution, i.e the unknown pixels are labelled either probable foreground or probable background.\n",
    "\n",
    "Its done depending on its relation with the other hard-labelled pixels in terms of color statistics (it is just like clustering).\n",
    "\n",
    "A graph is built from this pixel distribution. \n",
    "Nodes in the graphs are pixels. \n",
    "\n",
    "Additional two nodes are added, Source node and Sink node. Every foreground pixel is connected to Source node and every background pixel is connected to Sink node.\n",
    "\n",
    "The weights of edges connecting pixels to source node/end node are defined by the probability of a pixel being foreground/background. \n",
    "\n",
    "The weights between the pixels are defined by the edge information or pixel similarity. \n",
    "\n",
    "If there is a large difference in pixel color, the edge between them will get a low weight.\n",
    "\n",
    "Then a mincut algorithm is used to segment the graph. It cuts the graph into two separating source node and sink node with minimum cost function. \n",
    "\n",
    "The cost function is the sum of all weights of the edges that are cut. \n",
    "\n",
    "After the cut, all the pixels connected to Source node become foreground and those connected to Sink node become background.\n",
    "\n",
    "The process is continued until the classification converges.\n",
    "\n",
    "Most cases, this result in very fine Foreground and Background Segmentation.\n",
    "\n",
    "However if it fails then we just need to fine tune the mask and our image would again be automatically segmented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "auto_mask = np.zeros(src.shape[:2],np.uint8) ##intialize a mask image\n",
    "\n",
    "bgdModel = np.zeros((1,65),np.float64) ##intialize background and foreground model\n",
    "fgdModel = np.zeros((1,65),np.float64)\n",
    "if RectangleSelection:\n",
    "    print(x1, x2, y1, y2)\n",
    "    rect= (int(x1),int(y1),int(x2-x1),int(y2-y1))\n",
    "else:\n",
    "    rect = (10,10,270,80)\n",
    "### Utilze the OpenCv grab cut function to auto matically segement the object. -- \n",
    "### Grab cut utilizes the graph cut algorithm\n",
    "cv2.grabCut(src,auto_mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "mask2 = np.where((auto_mask==2)|(auto_mask==0),0,1).astype('uint8')\n",
    "dis_img = mask2[:,:,np.newaxis]\n",
    "#plt.figure(4)\n",
    "#plt.imshow(dis_img)\n",
    "width = (src.shape[1]+dis_img.shape[1]+2*margin+spacing)/dpi # inches\n",
    "height= (max(src.shape[0],dis_img.shape[0])+2*margin+spacing)/dpi\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(width,height), dpi=dpi)\n",
    "# show image\n",
    "ax[0].imshow(dis_img)\n",
    "ax[1].imshow(src)\n",
    "# define the foreground region\n",
    "ax[1].add_patch(Rectangle(rect[:2],rect[2], rect[3],linewidth=2,edgecolor='r',facecolor='none'))\n",
    "# show the mask \n",
    "ax[1].contour(mask2, [0.5], colors=\"yellow\", linewidths=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MIN_MATCH_COUNT = 10\n",
    "## Read Object Image and Image for Object Detection\n",
    "qry_src = cv2.imread('box.png')          # queryImage\n",
    "det_src = cv2.imread('box_in_scene.png') # trainImage\n",
    "\n",
    "width = (qry_src.shape[1]+det_src.shape[1]+2*margin+spacing)/dpi # inches\n",
    "height= (max(qry_src.shape[0],det_src.shape[0])+2*margin+spacing)/dpi\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(width,height), dpi=dpi)\n",
    "ax[0].imshow(qry_src)\n",
    "ax[1].imshow(det_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s find features in images ( here we use scale-invariant feature transform or SIFT [2]) and apply the ratio test to find the best matches.\n",
    "\n",
    "[2] Lowe, David G. \"Object recognition from local scale-invariant features.\" Proceedings of the seventh IEEE international conference on computer vision. Vol. 2. Ieee, 1999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT feature Detector\n",
    "sift = cv2.SIFT_create()\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(qry_src,None)\n",
    "kp2, des2 = sift.detectAndCompute(det_src,None)\n",
    "# Draw keypoints on the image\n",
    "kp1_img = cv2.drawKeypoints(qry_src, kp1, None, color=(255, 255, 0),flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "kp2_img = cv2.drawKeypoints(det_src, kp2, None, color=(255, 255, 0),flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "width = (kp1_img.shape[1]+kp2_img.shape[1]+2*margin+spacing)/dpi # inches\n",
    "height= (max(kp1_img.shape[0],kp2_img.shape[0])+2*margin+spacing)/dpi\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(width,height), dpi=dpi)\n",
    "ax[0].imshow(kp1_img)\n",
    "ax[1].imshow(kp2_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to match these features.\n",
    "\n",
    "For these we use a Fast Library for Approximate Nearest Neighbors (FLANN [3]). \n",
    "\n",
    "It contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. \n",
    "\n",
    "It works more faster than BFMatcher for large datasets. \n",
    "\n",
    "For FLANN based matcher, we need to pass two dictionaries which specifies the algorithm to be used, its related parameters etc. \n",
    "\n",
    "First one is IndexParams. \n",
    "\n",
    "For various algorithms, the information to be passed is explained in FLANN docs.\n",
    "\n",
    "Second dictionary is the SearchParams. \n",
    "\n",
    "It specifies the number of times the trees in the index should be recursively traversed. \n",
    "\n",
    "Higher values gives better precision, but also takes more time. \n",
    "\n",
    "[3] Muja, Marius, and David G. Lowe. \"Fast approximate nearest neighbors with automatic algorithm configuration.\" VISAPP (1) 2.331-340 (2009): 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inititate Flann based matcher\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks = 50)\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "matchesMask = [[1,0] for i in range(len(matches))]\n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    "                   singlePointColor = (255,0,0),\n",
    "                   matchesMask = matchesMask,\n",
    "                   flags = 0)\n",
    "# draw matches\n",
    "match_img = cv2.drawMatchesKnn(qry_src,kp1,det_src,kp2,matches,None,**draw_params)\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(match_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all the good matches as per Lowe's ratio test.\n",
    "good = []\n",
    "newmatchesMask = [[0,0] for i in range(len(matches))]\n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    "                   singlePointColor = (255,0,0),\n",
    "                   matchesMask = newmatchesMask,\n",
    "                   flags = 0)\n",
    "for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append(m)\n",
    "        newmatchesMask[i]=[1,0]\n",
    "# draw matches\n",
    "match_img = cv2.drawMatchesKnn(qry_src,kp1,det_src,kp2,matches,None,**draw_params)\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(match_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set a condition that atleast 10 matches (defined by `MIN_MATCH_COUNT`) are to be there to find the object. \n",
    "\n",
    "Otherwise simply show a message saying not enough matches are present.\n",
    "\n",
    "If enough matches are found, we extract the locations of matched keypoints in both the images. \n",
    "\n",
    "They are passed to find the perpective transformation. \n",
    "\n",
    "Once we get this 3x3 transformation matrix, we use it to transform the corners of queryImage to corresponding points in trainImage. Then we draw it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(good)>MIN_MATCH_COUNT:\n",
    "    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    matchesMask = mask.ravel().tolist()\n",
    "    h,w,_ = qry_src.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "    dst = cv2.perspectiveTransform(pts,M)\n",
    "    img2 = cv2.polylines(det_src,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "else:\n",
    "    print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_COUNT) )\n",
    "    matchesMask = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we draw our inliers (if successfully found the object) or matching keypoints (if failed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 2)\n",
    "res_img = cv2.drawMatches(qry_src,kp1,det_src,kp2,good,None,**draw_params)\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(res_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Task (4%)\n",
    "\n",
    "## Panorama Stitiching\n",
    " 1. Take multiple images of an interesting scene from your own camera with sufficient overlapping field of view between these images. Pan your camera through the scene such that multiple images have overlapping field of view however still have different scene content within each image.\n",
    " 2. Stitch them together to generate a panorama.\n",
    " 3. Use appropriate blending method to appropriatly blend them together (e.g. Poisson blending, ...).\n",
    "\n",
    "For further details on panorama stitching, we refer to Brown and Lowe [4]. Note that your solution is only supposed to use functions available in the `numpy`, `skimage`, and `cv2` packages. No worries, your solution does not have to be real-time capable.\n",
    "\n",
    "[4] Matthew Brown and David G. Lowe. 2007. Automatic Panoramic Image Stitching using Invariant Features. Int. J. Comput. Vision 74, 1 (August 2007), 59-73."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a45cccc4463853c93d05a566aff904cd63ca24a9d09bd79747d01abe0028757"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
